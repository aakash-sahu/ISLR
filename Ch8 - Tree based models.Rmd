---
title: "Chap_8-Tree based models"
author: "Aakash Sahu"
date: "April 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Decision Trees
Carseats data



```{r}
library(ISLR)
library(tree)
attach(Carseats)
hist(Sales)
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
View(Carseats)

```

```{r}
library(ggplot2)
ggplot(Carseats) +
  geom_histogram(aes(Sales), bins = 50)

ggplot(Carseats) +
  geom_freqpoly(aes(Sales))
```


```{r}
tree.carseats = tree(High ~ .-Sales, data = Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty =0)
```
```{r}
tree.carseats
```
Creating tree with training and test set

```{r}
set.seed(1011)
train = sample(1:nrow(Carseats), 250)
tree.carseats = tree(High ~ . - Sales, Carseats, subset = train)
#tree.carseats
plot(tree.carseats);text(tree.carseats, pretty =0)
tree.pred = predict(tree.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
(74+34)/150
```
Choosing tree size through cross validation


```{r}
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
summary(cv.carseats)
plot(cv.carseats)
prune.carseats = prune.misclass(tree.carseats, best = 13)
plot(prune.carseats);text(prune.carseats, pretty =0)
```
Evaluating on test data

```{r}
tree.pred = predict(prune.carseats, Carseats[-train,], type = "class")
with(Carseats[-train,], table(tree.pred, High))
(72+32)/150
```

Random Forest and Boosting
```{r}
library(randomForest)
library(MASS)
set.seed(101)
dim(Boston)
train = sample(1:nrow(Boston), 300)
?Boston
```

creating random forest with medv as response variable

```{r}
rf.boston = randomForest(medv ~ ., data = Boston, subset = train)
rf.boston
plot(rf.boston)
rf.boston$mse[200]
```

```{r}
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
  fit = randomForest(medv ~ ., data = Boston, subset = train, mtry = mtry, ntree = 400)
  oob.err[mtry]= fit$mse[400]
  pred = predict(fit, Boston[-train,])
  test.err[mtry]= with(Boston[-train,], mean((medv -pred)^2))
  cat(mtry, " ")
}

matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))

```
Boosting

```{r}
library(gbm)

boost.boston = gbm(medv ~ ., data = Boston[train,], distribution = "gaussian", n.trees = 10000 )
summary(boost.boston)
plot(boost.boston, i = "lstat")
plot(boost.boston, i = "rm")
```
test error as a fucntion of tree

```{r}
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees= n.trees)
dim(predmat)
berr = with(Boston[-train,], apply((predmat - medv)^2, 2, mean))
plot(n.trees,berr,pch=19,ylab="Mean Squared Error", xlab="# Trees",main="Boosting Test Error")
abline(h=min(test.err),col="red")
